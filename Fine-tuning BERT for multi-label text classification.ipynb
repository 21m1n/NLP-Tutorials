{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebad283-1998-4ace-b748-0e0063176622",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for multi-label text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb25e4-f9a0-43bb-95d4-200418287227",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff24e57-6613-42e6-8222-77e20ccaa3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a84d8d-6fb5-4557-b21e-ff5e439ed877",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ef05b-65a9-4f3e-803a-c6ca5c6bd6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 10:46:25,433 - INFO - the dataset distribution: {'train': 6838, 'test': 3259, 'validation': 886}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "\n",
    "logging.info(f\"the dataset distribution: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757cff43-fe3c-4ae3-ba57-01a6a72d8742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2017-En-21441',\n",
       " 'Tweet': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'anger': False,\n",
       " 'anticipation': True,\n",
       " 'disgust': False,\n",
       " 'fear': False,\n",
       " 'joy': False,\n",
       " 'love': False,\n",
       " 'optimism': True,\n",
       " 'pessimism': False,\n",
       " 'sadness': False,\n",
       " 'surprise': False,\n",
       " 'trust': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12254013-3e11-41a8-ad33-e133ab6d7ced",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5837d7da-4130-4dbc-a61b-477df9e66203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 10:46:27,125 - INFO - creating a mapping table for labels\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"creating a mapping table for labels\")\n",
    "\n",
    "labels = [label for label in dataset[\"train\"].features if label not in [\"ID\", \"Tweet\"]]\n",
    "\n",
    "label2id = {idx:label for idx, label in enumerate(labels)}\n",
    "id2label = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd04d06-e671-4c0c-af62-fbd651bd96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "num_labels = len(labels)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405bd23d-3d62-4650-9cf8-269dc1ec4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(\"\\#\", \" \", text) # remove hashtag # symbol only \n",
    "    text = re.sub(r\"\\\\n\", \" \", text) # remove newlines\n",
    "    text = text.lower() # text normalization\n",
    "    text = re.sub(\"\\@\\w+\", \" \", text) # remove @\n",
    "    text = re.sub('\\w*\\d\\w*', \" \", text) # remove digits\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text) # remove emoji\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), \" \", text) # remove punctuation\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove excessive white space \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1eaff64-72af-45b5-a113-b138f6b9701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 10:46:35,248 - INFO - measuring text length\n",
      "2023-09-11 10:46:35,396 - INFO - max length is 58\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"measuring text length\")\n",
    "\n",
    "train_text_len = []\n",
    "\n",
    "for text in dataset[\"train\"][\"Tweet\"]:\n",
    "    processed_text = text_preprocessing(text)\n",
    "    len_ = len(text.split(\" \"))\n",
    "    train_text_len.append(len_)\n",
    "\n",
    "logging.info(f\"max length is {max(train_text_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a8476e-75fb-4072-a12d-fedb3ddd79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "examples = dataset[\"train\"][0]\n",
    "item_label = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "for idx, label in enumerate(labels):\n",
    "    print(item_label[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d47efd-69b4-44e4-85a7-dd5c4ba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_text(item):\n",
    "\n",
    "    # preprocessing text\n",
    "    text = [text_preprocessing(t) for t in item[\"Tweet\"]]\n",
    "\n",
    "    # encoding text\n",
    "    encoding = tokenizer(text, \n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       max_length=max_length)\n",
    "\n",
    "    # initiate an empty matrix to store both text and labels\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "\n",
    "    # process labels\n",
    "    item_label = {label:item[label] for label in labels}\n",
    "\n",
    "    # fill the empty array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = item_label[label]\n",
    "\n",
    "    # converting to list\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53267b9d-3314-4246-bced-68d92fd4bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 10:46:53,026 - INFO - preprocessing text\n",
      "Map: 100%|█████████████████████████████████████| 886/886 [00:00<00:00, 12855.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"preprocessing text\")\n",
    "\n",
    "encoded_dataset = dataset.map(encoding_text, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6655c8-72cc-4590-9c8b-34d08532acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4737, 2003, 1037, 2091, 7909, 2006, 1037, 3291, 2017, 2089, 2196, 2031, 11830, 11527, 14354, 4105, 4737, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e51ed-0fca-4627-8906-d53ef7e121a5",
   "metadata": {},
   "source": [
    "# Model Training I: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e97aa78-0ae5-465e-b3de-80ff184f2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ae751b9-787e-4673-9c2a-33e23514c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27fbe806-e07c-4c2b-8b42-34b975940042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f9b3042-82da-4ec3-ac76-a9394b315b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4e8e2e0-a0cf-45dc-8861-af20ad6de1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 19:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322189</td>\n",
       "      <td>0.672363</td>\n",
       "      <td>0.769876</td>\n",
       "      <td>0.257336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.309953</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>0.787536</td>\n",
       "      <td>0.285553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.305427</td>\n",
       "      <td>0.707035</td>\n",
       "      <td>0.798385</td>\n",
       "      <td>0.291196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.313761</td>\n",
       "      <td>0.700223</td>\n",
       "      <td>0.796378</td>\n",
       "      <td>0.267494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.311049</td>\n",
       "      <td>0.702757</td>\n",
       "      <td>0.796239</td>\n",
       "      <td>0.283296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.27597292962475356, metrics={'train_runtime': 1185.2641, 'train_samples_per_second': 28.846, 'train_steps_per_second': 1.806, 'total_flos': 2249123476753920.0, 'train_loss': 0.27597292962475356, 'epoch': 5.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18f2d583-9af5-4efb-9505-9cd77dc220a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.305427223443985,\n",
       " 'eval_f1': 0.707035175879397,\n",
       " 'eval_roc_auc': 0.7983848635714869,\n",
       " 'eval_accuracy': 0.291196388261851,\n",
       " 'eval_runtime': 7.9125,\n",
       " 'eval_samples_per_second': 111.975,\n",
       " 'eval_steps_per_second': 7.077,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6dac5-6082-43ef-9395-c8580ce91b6f",
   "metadata": {},
   "source": [
    "# Model Training II: TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f72b36c-e70d-4813-bd4b-b9a7dd5b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_dataset = encoded_dataset.with_format(\"tf\") # not working\n",
    "\n",
    "tf_train = encoded_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val = encoded_dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21f255e6-481f-4a02-8715-65d91091ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                                problem_type=\"multi_label_classification\",\n",
    "                                                                num_labels=num_labels,\n",
    "                                                                id2label=id2label,\n",
    "                                                                label2id=label2id\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de76a86-a000-4780-b03b-1ed243e3f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"anger\": 0,\n",
      "    \"anticipation\": 1,\n",
      "    \"disgust\": 2,\n",
      "    \"fear\": 3,\n",
      "    \"joy\": 4,\n",
      "    \"love\": 5,\n",
      "    \"optimism\": 6,\n",
      "    \"pessimism\": 7,\n",
      "    \"sadness\": 8,\n",
      "    \"surprise\": 9,\n",
      "    \"trust\": 10\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": \"anger\",\n",
      "    \"1\": \"anticipation\",\n",
      "    \"2\": \"disgust\",\n",
      "    \"3\": \"fear\",\n",
      "    \"4\": \"joy\",\n",
      "    \"5\": \"love\",\n",
      "    \"6\": \"optimism\",\n",
      "    \"7\": \"pessimism\",\n",
      "    \"8\": \"sadness\",\n",
      "    \"9\": \"surprise\",\n",
      "    \"10\": \"trust\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.32.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3039f07-0bff-40e0-b71a-6f747227a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training: instantiate optimizer, loss and learning rate schedule \n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5) # using legacy optimizer on M1/M2 MBP\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "tf_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcb63db7-bb4e-4543-9ad1-c556755f0c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 14:49:11.674959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/428 [==============================] - ETA: 0s - loss: 0.6246 - categorical_accuracy: 0.1840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 14:54:04.409595: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/428 [==============================] - 317s 707ms/step - loss: 0.6246 - categorical_accuracy: 0.1840 - val_loss: 0.4736 - val_categorical_accuracy: 0.3352\n",
      "Epoch 2/5\n",
      "428/428 [==============================] - 281s 653ms/step - loss: 0.4864 - categorical_accuracy: 0.2379 - val_loss: 0.4518 - val_categorical_accuracy: 0.3905\n",
      "Epoch 3/5\n",
      "428/428 [==============================] - 274s 639ms/step - loss: 0.4477 - categorical_accuracy: 0.3342 - val_loss: 0.3807 - val_categorical_accuracy: 0.4819\n",
      "Epoch 4/5\n",
      "428/428 [==============================] - 273s 637ms/step - loss: 0.4159 - categorical_accuracy: 0.4223 - val_loss: 0.3821 - val_categorical_accuracy: 0.1919\n",
      "Epoch 5/5\n",
      "428/428 [==============================] - 279s 651ms/step - loss: 0.4105 - categorical_accuracy: 0.3738 - val_loss: 0.4055 - val_categorical_accuracy: 0.4041\n"
     ]
    }
   ],
   "source": [
    "history = tf_model.fit(tf_train, \n",
    "                       validation_data=tf_val,\n",
    "                       epochs=5,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353dd15-d573-4bca-ac45-eb0776b7ad81",
   "metadata": {},
   "source": [
    "# Model Training III: TensorFlow with pre-trained BERT\n",
    "\n",
    "reference: https://github.com/huggingface/transformers/issues/1465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07cb8652-a552-4814-84e7-a8b07b0d58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                # kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17bafc57-e0e2-4919-81ea-5f832ca03e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, TFBertModel\n",
    "\n",
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        # self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.bert = TFBertModel.from_pretrained(model_id).layers[0]\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                # kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name=\"classifier\",\n",
    "                                                activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get(\"training\", False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits, ) + output[2:]\n",
    "\n",
    "        return outputs # logits, (hidden_states), (attentions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04c65a21-b449-4950-9b5d-1f0ec124aef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n\nYou have to specify either input_ids or inputs_embeds\n\nCall arguments received by layer 'bert' (type TFBertMainLayer):\n  • input_ids={}\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_model2 \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertForMultilabelClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2908\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2906\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2911\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1138\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Set the serving spec quickly to ensure that Keras doesn't use the specific dummy input shapes as the spec\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# Setting it in build() allows users to override the shape when loading a non-pretrained model from config\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_save_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature)\n\u001b[0;32m-> 1138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[74], line 15\u001b[0m, in \u001b[0;36mTFBertForMultilabelClassification.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output, training\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/transformers/models/bert/modeling_tf_bert.py:764\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    762\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m shape_list(inputs_embeds)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    766\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n\nYou have to specify either input_ids or inputs_embeds\n\nCall arguments received by layer 'bert' (type TFBertMainLayer):\n  • input_ids={}\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "tf_model2 = TFBertForMultilabelClassification.from_pretrained(model_id, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb5135-757c-4634-b61c-547d551bd924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be58fdc-3651-47d8-8412-94e328a13914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training: instantiate optimizer, loss and learning rate schedule \n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5) # using legacy optimizer on M1/M2 MBP\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "tf_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
