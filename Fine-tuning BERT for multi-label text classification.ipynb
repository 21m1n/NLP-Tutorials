{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcb25e4-f9a0-43bb-95d4-200418287227",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cff24e57-6613-42e6-8222-77e20ccaa3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.11/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a84d8d-6fb5-4557-b21e-ff5e439ed877",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "686ef05b-65a9-4f3e-803a-c6ca5c6bd6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 14:08:25,924 - INFO - the dataset distribution: {'train': 6838, 'test': 3259, 'validation': 886}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "\n",
    "logging.info(f\"the dataset distribution: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757cff43-fe3c-4ae3-ba57-01a6a72d8742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2017-En-21441',\n",
       " 'Tweet': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'anger': False,\n",
       " 'anticipation': True,\n",
       " 'disgust': False,\n",
       " 'fear': False,\n",
       " 'joy': False,\n",
       " 'love': False,\n",
       " 'optimism': True,\n",
       " 'pessimism': False,\n",
       " 'sadness': False,\n",
       " 'surprise': False,\n",
       " 'trust': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12254013-3e11-41a8-ad33-e133ab6d7ced",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5837d7da-4130-4dbc-a61b-477df9e66203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 14:08:25,934 - INFO - creating a mapping table for labels\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"creating a mapping table for labels\")\n",
    "\n",
    "labels = [label for label in dataset[\"train\"].features if label not in [\"ID\", \"Tweet\"]]\n",
    "\n",
    "label2id = {idx:label for idx, label in enumerate(labels)}\n",
    "id2label = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acd04d06-e671-4c0c-af62-fbd651bd96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "num_labels = len(labels)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "405bd23d-3d62-4650-9cf8-269dc1ec4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(\"\\#\", \" \", text) # remove hashtag # symbol only \n",
    "    text = re.sub(r\"\\\\n\", \" \", text) # remove newlines\n",
    "    text = text.lower() # text normalization\n",
    "    text = re.sub(\"\\@\\w+\", \" \", text) # remove @\n",
    "    text = re.sub('\\w*\\d\\w*', \" \", text) # remove digits\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text) # remove emoji\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), \" \", text) # remove punctuation\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove excessive white space \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1eaff64-72af-45b5-a113-b138f6b9701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 14:08:29,038 - INFO - measuring text length\n",
      "2023-08-27 14:08:29,191 - INFO - max length is 58\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"measuring text length\")\n",
    "\n",
    "train_text_len = []\n",
    "\n",
    "for text in dataset[\"train\"][\"Tweet\"]:\n",
    "    processed_text = text_preprocessing(text)\n",
    "    len_ = len(text.split(\" \"))\n",
    "    train_text_len.append(len_)\n",
    "\n",
    "logging.info(f\"max length is {max(train_text_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34a8476e-75fb-4072-a12d-fedb3ddd79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "examples = dataset[\"train\"][0]\n",
    "item_label = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "for idx, label in enumerate(labels):\n",
    "    print(item_label[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91d47efd-69b4-44e4-85a7-dd5c4ba2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_text(item):\n",
    "\n",
    "    # preprocessing text\n",
    "    text = [text_preprocessing(t) for t in item[\"Tweet\"]]\n",
    "\n",
    "    # encoding text\n",
    "    encoding = tokenizer(text, \n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       max_length=max_length)\n",
    "\n",
    "    # initiate an empty matrix to store both text and labels\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "\n",
    "    # process labels\n",
    "    item_label = {label:item[label] for label in labels}\n",
    "\n",
    "    # fill the empty array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = item_label[label]\n",
    "\n",
    "    # converting to list\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53267b9d-3314-4246-bced-68d92fd4bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 14:08:31,895 - INFO - preprocessing text\n",
      "Map: 100%|████████████████████████| 3259/3259 [00:00<00:00, 18236.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"preprocessing text\")\n",
    "\n",
    "encoded_dataset = dataset.map(encoding_text, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad6655c8-72cc-4590-9c8b-34d08532acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4737, 2003, 1037, 2091, 7909, 2006, 1037, 3291, 2017, 2089, 2196, 2031, 11830, 11527, 14354, 4105, 4737, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e51ed-0fca-4627-8906-d53ef7e121a5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e97aa78-0ae5-465e-b3de-80ff184f2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ae751b9-787e-4673-9c2a-33e23514c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27fbe806-e07c-4c2b-8b42-34b975940042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f9b3042-82da-4ec3-ac76-a9394b315b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4e8e2e0-a0cf-45dc-8861-af20ad6de1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 19:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322189</td>\n",
       "      <td>0.672363</td>\n",
       "      <td>0.769876</td>\n",
       "      <td>0.257336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.309953</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>0.787536</td>\n",
       "      <td>0.285553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.305427</td>\n",
       "      <td>0.707035</td>\n",
       "      <td>0.798385</td>\n",
       "      <td>0.291196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.313761</td>\n",
       "      <td>0.700223</td>\n",
       "      <td>0.796378</td>\n",
       "      <td>0.267494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.311049</td>\n",
       "      <td>0.702757</td>\n",
       "      <td>0.796239</td>\n",
       "      <td>0.283296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.27597292962475356, metrics={'train_runtime': 1185.2641, 'train_samples_per_second': 28.846, 'train_steps_per_second': 1.806, 'total_flos': 2249123476753920.0, 'train_loss': 0.27597292962475356, 'epoch': 5.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18f2d583-9af5-4efb-9505-9cd77dc220a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.305427223443985,\n",
       " 'eval_f1': 0.707035175879397,\n",
       " 'eval_roc_auc': 0.7983848635714869,\n",
       " 'eval_accuracy': 0.291196388261851,\n",
       " 'eval_runtime': 7.9125,\n",
       " 'eval_samples_per_second': 111.975,\n",
       " 'eval_steps_per_second': 7.077,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082fabc-e286-4a9a-ad16-859e9e8376f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
